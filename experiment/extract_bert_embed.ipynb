{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn import preprocessing as pp\n",
    "#from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler, BatchSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('data/nlp_prepared_bert.csv')\n",
    "all_data.html_raw = all_data.html_raw.fillna(\"\").astype(str)\n",
    "train_set = all_data[all_data[\"data_type\"] == \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>goal</th>\n",
       "      <th>country</th>\n",
       "      <th>duration</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>html_content</th>\n",
       "      <th>state</th>\n",
       "      <th>data_type</th>\n",
       "      <th>html_compiled</th>\n",
       "      <th>html_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4001-5000</td>\n",
       "      <td>CH</td>\n",
       "      <td>29</td>\n",
       "      <td>publishing</td>\n",
       "      <td>young adult</td>\n",
       "      <td>&lt;div class=\"contents\"&gt;&lt;div&gt;&lt;span class=\"bold\"&gt;...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;div&gt;&lt;div&gt;&lt;span&gt;Mark Saggia&lt;/span&gt; is an Itali...</td>\n",
       "      <td>Mark Saggia is an Italian writer who emigrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3001-4000</td>\n",
       "      <td>NL</td>\n",
       "      <td>34</td>\n",
       "      <td>fashion</td>\n",
       "      <td>ready-to-wear</td>\n",
       "      <td>&lt;div class=\"contents\"&gt;&lt;div&gt;&lt;h1 class=\"page-anc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;div&gt;&lt;div&gt;&lt;h1&gt;Hello, I am Augustinas. I am a g...</td>\n",
       "      <td>Hello, I am Augustinas. I am a graphic designe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19001-20000</td>\n",
       "      <td>US</td>\n",
       "      <td>30</td>\n",
       "      <td>food</td>\n",
       "      <td>spaces</td>\n",
       "      <td>&lt;div class=\"contents\"&gt;&lt;div&gt;&lt;p&gt; As our society ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;div&gt;&lt;div&gt;&lt;p&gt; As our society begins to wake up...</td>\n",
       "      <td>As our society begins to wake up from the han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2001-3000</td>\n",
       "      <td>US</td>\n",
       "      <td>41</td>\n",
       "      <td>technology</td>\n",
       "      <td>3d printing</td>\n",
       "      <td>&lt;div class=\"contents\"&gt;&lt;div&gt;&lt;p&gt;My name is Donal...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;div&gt;&lt;div&gt;&lt;p&gt;My name is Donald Osborne and I a...</td>\n",
       "      <td>My name is Donald Osborne and I am an entrepre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2001-3000</td>\n",
       "      <td>GB</td>\n",
       "      <td>29</td>\n",
       "      <td>technology</td>\n",
       "      <td>diy electronics</td>\n",
       "      <td>&lt;div class=\"contents\"&gt;&lt;div&gt;&lt;div class=\"templat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;div&gt;&lt;div&gt;&lt;div&gt; &lt;figure&gt; &lt;img&gt; &lt;/figure&gt; &lt;/div...</td>\n",
       "      <td>We all love to play, don't we! No matter the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         goal country  duration   category1        category2  \\\n",
       "0   0    4001-5000      CH        29  publishing      young adult   \n",
       "1   1    3001-4000      NL        34     fashion    ready-to-wear   \n",
       "2   2  19001-20000      US        30        food           spaces   \n",
       "3   3    2001-3000      US        41  technology      3d printing   \n",
       "4   4    2001-3000      GB        29  technology  diy electronics   \n",
       "\n",
       "                                        html_content  state data_type  \\\n",
       "0  <div class=\"contents\"><div><span class=\"bold\">...    0.0     train   \n",
       "1  <div class=\"contents\"><div><h1 class=\"page-anc...    0.0     train   \n",
       "2  <div class=\"contents\"><div><p> As our society ...    0.0     train   \n",
       "3  <div class=\"contents\"><div><p>My name is Donal...    0.0     train   \n",
       "4  <div class=\"contents\"><div><div class=\"templat...    1.0     train   \n",
       "\n",
       "                                       html_compiled  \\\n",
       "0  <div><div><span>Mark Saggia</span> is an Itali...   \n",
       "1  <div><div><h1>Hello, I am Augustinas. I am a g...   \n",
       "2  <div><div><p> As our society begins to wake up...   \n",
       "3  <div><div><p>My name is Donald Osborne and I a...   \n",
       "4  <div><div><div> <figure> <img> </figure> </div...   \n",
       "\n",
       "                                            html_raw  \n",
       "0  Mark Saggia is an Italian writer who emigrated...  \n",
       "1  Hello, I am Augustinas. I am a graphic designe...  \n",
       "2   As our society begins to wake up from the han...  \n",
       "3  My name is Donald Osborne and I am an entrepre...  \n",
       "4   We all love to play, don't we! No matter the ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HtmlDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform, stage):\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.stage = stage\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        html_text = self.csv_file.html_raw.iloc[idx]\n",
    "        html_text = str(html_text)\n",
    "        if self.stage == \"train\":\n",
    "            label = self.csv_file.state.iloc[idx]\n",
    "        elif self.stage == \"eval\":\n",
    "            label = self.csv_file.id.iloc[idx]\n",
    "            \n",
    "        if self.transform:\n",
    "            html_text = self.transform(html_text)\n",
    "            \n",
    "        return html_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transform\n",
    "class BERT_Tokenize(object):\n",
    "    def __init__(self, model_type, max_len):\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if model_type == \"BERT\" or model_type == \"TAPTBERT\":\n",
    "            from transformers import BertTokenizer, BertForSequenceClassification\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            \n",
    "        elif model_type == \"ALBERT\":\n",
    "            from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "            self.bert_tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "            \n",
    "        elif model_type == \"XLNET\":\n",
    "            from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "            self.bert_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "        \n",
    "        elif model_type == \"ROBERTA\":\n",
    "            from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "            self.bert_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "        elif model_type == \"csROBERTA\":\n",
    "            from transformers import AutoTokenizer, AutoModel\n",
    "            self.bert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/cs_roberta_base\")\n",
    "            \n",
    "        elif model_type == \"XLMROBERTA\":\n",
    "            from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "            self.bert_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "            \n",
    "        elif model_type == \"ELECTRA\":\n",
    "            from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "            self.bert_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n",
    "            \n",
    "    \n",
    "    def __call__(self,text):\n",
    "        inputs = self.bert_tokenizer.encode_plus(\n",
    "                        text,                       # Sentence to encode.\n",
    "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = self.max_len,  # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  # Construct attn. masks.\n",
    "                        return_tensors = \"pt\"\n",
    "                   )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        del text, inputs\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, model_type, out_dim, stage):\n",
    "        super(BertHead, self).__init__()\n",
    "        if model_type == \"ALBERT\":\n",
    "            from transformers import AlbertTokenizer, AlbertModel\n",
    "            self.base_model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "            \n",
    "        elif model_type == \"BERT\":\n",
    "            from transformers import BertTokenizer, BertModel\n",
    "            self.base_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "            \n",
    "        elif model_type == \"XLNET\":\n",
    "            from transformers import XLNetTokenizer, XLNetModel\n",
    "            self.base_model = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
    "            \n",
    "        elif model_type == \"ROBERTA\":\n",
    "            from transformers import RobertaTokenizer, RobertaModel\n",
    "            self.base_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "        elif model_type == \"XLMROBERTA\":\n",
    "            from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "            self.base_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        \n",
    "        elif model_type == \"ELECTRA\":\n",
    "            from transformers import ElectraTokenizer, ElectraModel\n",
    "            self.base_model = ElectraModel.from_pretrained(\"google/electra-base-discriminator\")\n",
    "       \n",
    "        elif model_type == \"TAPTBERT\":\n",
    "            from transformers import AutoModel, AutoConfig\n",
    "            config = AutoConfig.from_pretrained(\"config.json\")\n",
    "            self.base_model = AutoModel.from_pretrained(\"pytorch_model.bin\", config=config)\n",
    "        \n",
    "        self.stage = stage\n",
    "        dropout = 0.2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 768), nn.ReLU(), nn.Dropout(p=dropout),\n",
    "            nn.Linear(768, 768), nn.ReLU(), nn.Dropout(p=dropout),\n",
    "            nn.Linear(768, out_dim))\n",
    "         \n",
    "        \"\"\"\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ids, mask = x\n",
    "        x = self.base_model(input_ids=torch.squeeze(ids,dim=1), attention_mask=torch.squeeze(mask, dim=1))\n",
    "        if self.stage == \"train\":\n",
    "            x = self.classifier(x[1])\n",
    "            preds = torch.sigmoid(x)\n",
    "        else:\n",
    "            preds = torch.tensor(x[1])\n",
    "            \n",
    "        del ids, mask\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embed_output():\n",
    "    # config\n",
    "    csv_file = all_data\n",
    "    model_type = \"BERT\"\n",
    "    max_length = 512\n",
    "    bert_tokenizer = BERT_Tokenize(model_type, max_length)\n",
    "    transform = bert_tokenizer\n",
    "    out_dim = 1\n",
    "    batch_size = 16\n",
    "    num_workers = 4\n",
    "    stage = \"eval\"\n",
    "    #ckpt_path = \"../input/bert-pretrained-ckptcategory2/epoch0.ckpt\"\n",
    "    \n",
    "    \n",
    "    train_dataset = HtmlDataset(csv_file=csv_file,\n",
    "                                transform=bert_tokenizer,\n",
    "                                stage=stage)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=num_workers)\n",
    "    model = BertHead(model_type=model_type,\n",
    "                    out_dim=out_dim,\n",
    "                    stage=stage)\n",
    "    #check_point = torch.load(ckpt_path)\n",
    "    #model.load_state_dict(check_point[\"state_dict\"], strict=True)\n",
    "    \n",
    "    \n",
    "    preds = torch.tensor([])\n",
    "    ids = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for x, t in tqdm(train_dataloader):\n",
    "        x[0], x[1] = x[0].to(device), x[1].to(device)\n",
    "        x = model(x)\n",
    "        preds = torch.cat((preds, x.to(\"cpu\")), dim=0)\n",
    "        ids += list(map(int, t.to(\"cpu\")))\n",
    "        \n",
    "    preds = torch.squeeze(preds)\n",
    "    preds = preds.tolist()\n",
    "    prediction_df = pd.DataFrame(preds)\n",
    "    rename_dict = {}\n",
    "    for n in range(768):\n",
    "        rename_dict[n] = f\"bert_{n}\"\n",
    "    prediction_df = prediction_df.rename(columns=rename_dict)\n",
    "    prediction_df[\"id\"] = ids\n",
    "    \n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "embed_out_df = extract_embed_output()\n",
    "embed_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
