{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn import preprocessing as pp\n",
    "#from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler, BatchSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFWholeDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer=None):\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.target_cols = TARGET_COLS\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        features = self.csv_file[self.feature_cols].iloc[idx]\n",
    "        features = torch.FloatTensor(features)    \n",
    "        html_text = self.csv_file.html_raw.iloc[idx]\n",
    "        if self.tokenizer:\n",
    "            html_text = self.tokenizer(html_text)\n",
    "            \n",
    "        try:\n",
    "            target = self.csv_file[self.target_cols].iloc[idx]\n",
    "            target = torch.tensor(target)\n",
    "            # train_step\n",
    "        except:\n",
    "            target = self.csv_file[\"id\"].iloc[idx]\n",
    "            # test_step\n",
    "        \n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "            \n",
    "        return features, html_text, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transform\n",
    "class BERT_Tokenize(object):\n",
    "    def __init__(self, model_type, max_len):\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if model_type == \"BERT\" or model_type == \"TAPTBERT\":\n",
    "            from transformers import BertTokenizer, BertForSequenceClassification\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            \n",
    "        elif model_type == \"ALBERT\":\n",
    "            from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "            self.bert_tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "            \n",
    "        elif model_type == \"XLNET\":\n",
    "            from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "            self.bert_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "        \n",
    "        elif model_type == \"ROBERTA\":\n",
    "            from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "            self.bert_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "        elif model_type == \"csROBERTA\":\n",
    "            from transformers import AutoTokenizer, AutoModel\n",
    "            self.bert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/cs_roberta_base\")\n",
    "            \n",
    "        elif model_type == \"XLMROBERTA\":\n",
    "            from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "            self.bert_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "            \n",
    "        elif model_type == \"ELECTRA\":\n",
    "            from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "            self.bert_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n",
    "            \n",
    "    \n",
    "    def __call__(self,text):\n",
    "        inputs = self.bert_tokenizer.encode_plus(\n",
    "                        text,                       # Sentence to encode.\n",
    "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = self.max_len,  # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  # Construct attn. masks.\n",
    "                   )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        del text, inputs\n",
    "        return np.array(ids), np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, model_type, num_classes):\n",
    "        super(BertHead, self).__init__()\n",
    "        self.model_type = True if model_type == \"TAPTBERT\" else False\n",
    "        \n",
    "        if model_type == \"ALBERT\":\n",
    "            from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "            self.base_model = AlbertForSequenceClassification.from_pretrained(\n",
    "                \"albert-base-v2\",early_stopping=False,num_labels=num_classes)\n",
    "            \n",
    "        elif model_type == \"BERT\":\n",
    "            from transformers import BertTokenizer, BertForSequenceClassification\n",
    "            self.base_model = BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\",early_stopping=False,num_labels=num_classes)\n",
    "            \n",
    "        elif model_type == \"XLNET\":\n",
    "            from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "            self.base_model = XLNetForSequenceClassification.from_pretrained(\n",
    "                \"xlnet-base-cased\",early_stopping=False,num_labels=num_classes)\n",
    "            \n",
    "        elif model_type == \"ROBERTA\":\n",
    "            from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "            self.base_model = RobertaForSequenceClassification.from_pretrained(\n",
    "                \"roberta-base\",early_stopping=False,num_labels=num_classes)\n",
    "        \n",
    "        elif model_type == \"XLMROBERTA\":\n",
    "            from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "            self.base_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "                \"xlm-roberta-base\", num_labels=num_classes)\n",
    "        \n",
    "        elif model_type == \"ELECTRA\":\n",
    "            from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "            self.base_model = ElectraForSequenceClassification.from_pretrained(\n",
    "                \"google/electra-base-discriminator\", num_labels=num_classes)\n",
    "       \n",
    "        elif model_type == \"TAPTBERT\":\n",
    "            from transformers import AutoModel, AutoConfig\n",
    "            config = AutoConfig.from_pretrained(\"../input/tapt-v2/config.json\")\n",
    "            self.base_model = AutoModel.from_pretrained(\"../input/tapt-v2/pytorch_model.bin\", config=config)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(768, 768), nn.ReLU(), nn.Dropout(p=0.1),\n",
    "                nn.Linear(768, 768), nn.ReLU(), nn.Dropout(p=0.1),\n",
    "                nn.Linear(768, num_classes))\n",
    "    \"\"\"\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ids, mask = x\n",
    "        \n",
    "        if self.model_type:\n",
    "            x = self.base_model(input_ids=ids, attention_mask=mask)\n",
    "            x = self.classifier(x[1])\n",
    "            preds = x\n",
    "        else:\n",
    "            x = self.base_model(input_ids=ids, attention_mask=mask, labels=None)\n",
    "            preds = x[0]\n",
    "        \n",
    "        preds = F.sigmoid(preds)\n",
    "        del ids, mask, x\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNet(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, dropout, num_classes):\n",
    "        super(TabularNet, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(dropout*(2/5))\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size*2))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size*2, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.layer4 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.dense3(x))\n",
    "        \n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeNet(nn.Module):\n",
    "    def __init__(self, num_input, hidden_size, dropout, num_classes):\n",
    "        super(MergeNet, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT based model\n",
    "class BertModule(pl.LightningModule):\n",
    "    def __init__(self, num_features, model_type, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.berthead = BertHead(model_type=model_type, num_classes=num_classes)\n",
    "        self.tabularnet = TabularNet(num_features, hidden_size, dropout, num_classes)\n",
    "        self.mergenet = MergeNet(num_input, hidden_size, dropout, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        tab_input, bert_input, target = data\n",
    "        bert_prob = self.bert_head(bert_input)\n",
    "        tabular_prob = self.tabularnet(tab_input)\n",
    "        merge_input = torch.cat([tabular_prob, bert_prob], axis=1)\n",
    "        preds = self.mergenet(merge_input)\n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        pred = self.forward(x)\n",
    "        loss = self.criterion(pred, t)\n",
    "        acc = self.metric(pred, t)\n",
    "        # you should define log as {\"tag_name/log_name\"}\n",
    "        tensorboard_logs = {'train/train_loss': loss, \"train/train_acc\": acc}\n",
    "        return {\"loss\": loss, \"acc\": acc, \"logs\": tensorboard_logs, \"progress_bar\": tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        pred = self.forward(x)\n",
    "        loss = self.criterion(pred, t)\n",
    "        acc = self.metric(pred, t)\n",
    "        logs = {\"val_loss\": loss, \"val_acc\": acc}\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc, \"progress_bar\": logs}\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([torch.tensor(x['val_acc']) for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val/avg_loss': avg_loss, \"val/avg_acc\": avg_acc}\n",
    "        print(f\"val_loss: {avg_loss}, val_acc: {avg_acc}\")\n",
    "        # you should call back as name \"val_loss\" to using the Early-Stopping\n",
    "        return {'val_loss': avg_loss, \"val_acc\": avg_acc, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=1e-5)\n",
    "        \"\"\"\n",
    "        scheduler = {\"scheduler\": \n",
    "                     optimizers.lr_scheduler.CosineAnnealingLR(\n",
    "                        optimizer, T_max=10),\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"monitor\": \"val_loss\"}\n",
    "        \"\"\"\n",
    "        return optimizer#[optimizer], [scheduler]\n",
    "    \n",
    "    \n",
    "    def criterion(self, pred, t):\n",
    "        pred = pred.view(-1)\n",
    "        pred = pred.float()\n",
    "        t = t.float()\n",
    "        return F.binary_cross_entropy(pred,t)\n",
    "    \n",
    "    \n",
    "    def metric(self, pred, t):\n",
    "        pred = pred.view(-1)\n",
    "        pred = pred.float()\n",
    "        t = t.float()\n",
    "        pred = torch.where(pred<0.5, 0, 1)    \n",
    "        t, pred = t.to(\"cpu\"), pred.to(\"cpu\").detach().numpy()\n",
    "        return f1_score(y_true=t, y_pred=pred, average='binary', sample_weight=None, zero_division='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFhtmlDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_file, transform, split_rate, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.split_rate = split_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = self.csv_file\n",
    "        n_samples = len(dataset)\n",
    "        n_train = int(n_samples * 0.8)\n",
    "        n_val = n_samples - n_train\n",
    "        train_dataset, val_dataset = train_test_split(dataset,  train_size=n_train, test_size=n_val)\n",
    "        \n",
    "        self.train_dataset = HtmlDataset(csv_file=train_dataset, transform=self.transform)\n",
    "        self.val_dataset = HtmlDataset(csv_file=val_dataset, transform=self.transform)\n",
    "        \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          drop_last=True,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=True)\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          drop_last=True,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # trainer config\n",
    "    epochs = 400\n",
    "    output_path = './'\n",
    "    \n",
    "    # data module config\n",
    "    csv_file = train\n",
    "    model_type = \"BERT\"\n",
    "    max_length = 512\n",
    "    bert_tokenizer = BERT_Tokenize(model_type, max_length)\n",
    "    transform = bert_tokenizer\n",
    "    split_rate = 0.8\n",
    "    batch_size = 16\n",
    "    num_workers = 4\n",
    "    \n",
    "    # model config\n",
    "    num_classes = 1\n",
    "    \n",
    "    # early stopping config\n",
    "    patience = 5\n",
    "    \n",
    "    cf = CFhtmlDataModule(csv_file, transform, split_rate, batch_size, num_workers)\n",
    "    model = BertModule(model_type, num_classes)\n",
    "    \n",
    "    early_stopping = EarlyStopping('val_loss', patience=patience, verbose=True)\n",
    "    trainer = Trainer(\n",
    "            max_epochs=epochs,\n",
    "            weights_save_path=output_path,\n",
    "            gpus = 1 if torch.cuda.is_available() else None,\n",
    "            callbacks=[early_stopping]\n",
    "            #accumulate_grad_batches=1\n",
    "            # use_amp=False,\n",
    "        )\n",
    "        \n",
    "    trainer.fit(model, cf)\n",
    "    torch.cuda.empty_cache()\n",
    "    # TO DO: use model.apply(weights_init) instead of torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
